#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Embeddingæ£€ç´¢æµ‹è¯•ç”¨ä¾‹
ä½¿ç”¨å‘é‡åµŒå…¥å’Œä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œæ–‡æ¡£æ£€ç´¢
"""

import numpy as np
from typing import List, Dict, Tuple
import jieba
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

class EmbeddingRetriever:
    """
    åŸºäºå‘é‡åµŒå…¥çš„æ£€ç´¢å™¨
    ä½¿ç”¨TF-IDFå‘é‡åŒ–å’Œä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œæ–‡æ¡£æ£€ç´¢
    """
    
    def __init__(self, max_features: int = 1000, min_df: int = 1, max_df: float = 0.95):
        """
        åˆå§‹åŒ–Embeddingæ£€ç´¢å™¨
        
        Args:
            max_features: æœ€å¤§ç‰¹å¾æ•°é‡
            min_df: æœ€å°æ–‡æ¡£é¢‘ç‡
            max_df: æœ€å¤§æ–‡æ¡£é¢‘ç‡æ¯”ä¾‹
        """
        self.max_features = max_features
        self.min_df = min_df
        self.max_df = max_df
        self.vectorizer = None
        self.doc_vectors = None
        self.documents = []
        
    def preprocess_text(self, text: str) -> str:
        """
        æ–‡æœ¬é¢„å¤„ç†ï¼šåˆ†è¯ã€å»é™¤æ ‡ç‚¹ç¬¦å·
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬å­—ç¬¦ä¸²
        """
        # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        
        # ä¸­æ–‡åˆ†è¯
        words = jieba.lcut(text.lower())
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’Œå•å­—ç¬¦ï¼Œè¿”å›ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²
        words = [word.strip() for word in words if len(word.strip()) > 1]
        
        return ' '.join(words)
    
    def build_index(self, documents: List[str]):
        """
        æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        self.documents = documents
        
        # é¢„å¤„ç†æ‰€æœ‰æ–‡æ¡£
        processed_docs = [self.preprocess_text(doc) for doc in documents]
        
        # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=self.max_features,
            min_df=self.min_df,
            max_df=self.max_df,
            token_pattern=r'\b\w+\b'  # æ”¯æŒä¸­æ–‡è¯æ±‡
        )
        
        # æ„å»ºæ–‡æ¡£å‘é‡çŸ©é˜µ
        self.doc_vectors = self.vectorizer.fit_transform(processed_docs)
        
        print(f"ğŸ“Š å‘é‡ç»´åº¦: {self.doc_vectors.shape}")
        print(f"ğŸ“ è¯æ±‡è¡¨å¤§å°: {len(self.vectorizer.vocabulary_)}")
    
    def get_query_vector(self, query: str) -> np.ndarray:
        """
        å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            æŸ¥è¯¢å‘é‡
        """
        processed_query = self.preprocess_text(query)
        query_vector = self.vectorizer.transform([processed_query])
        return query_vector
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float, str]]:
        """
        æ‰§è¡Œå‘é‡æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            (æ–‡æ¡£ç´¢å¼•, ç›¸ä¼¼åº¦åˆ†æ•°, æ–‡æ¡£å†…å®¹)çš„åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        # è·å–æŸ¥è¯¢å‘é‡
        query_vector = self.get_query_vector(query)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()
        
        # è·å–æ’åºåçš„ç´¢å¼•
        sorted_indices = np.argsort(similarities)[::-1]
        
        # æ„å»ºç»“æœåˆ—è¡¨
        results = []
        for i in sorted_indices[:top_k]:
            results.append((i, similarities[i], self.documents[i]))
        
        return results
    
    def get_feature_weights(self, query: str, top_features: int = 10) -> List[Tuple[str, float]]:
        """
        è·å–æŸ¥è¯¢ä¸­æƒé‡æœ€é«˜çš„ç‰¹å¾è¯
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_features: è¿”å›å‰Nä¸ªç‰¹å¾
            
        Returns:
            (ç‰¹å¾è¯, æƒé‡)çš„åˆ—è¡¨
        """
        query_vector = self.get_query_vector(query)
        feature_names = self.vectorizer.get_feature_names_out()
        
        # è·å–éé›¶ç‰¹å¾åŠå…¶æƒé‡
        feature_weights = []
        for idx in query_vector.nonzero()[1]:
            weight = query_vector[0, idx]
            feature_weights.append((feature_names[idx], weight))
        
        # æŒ‰æƒé‡é™åºæ’åº
        feature_weights.sort(key=lambda x: x[1], reverse=True)
        
        return feature_weights[:top_features]

class SimpleWordEmbeddingRetriever:
    """
    ç®€å•çš„è¯å‘é‡æ£€ç´¢å™¨
    ä½¿ç”¨è¯é¢‘å‘é‡å’Œä½™å¼¦ç›¸ä¼¼åº¦
    """
    
    def __init__(self):
        self.vocabulary = set()
        self.documents = []
        self.doc_vectors = []
        self.vocab_to_idx = {}
    
    def preprocess_text(self, text: str) -> List[str]:
        """
        æ–‡æœ¬é¢„å¤„ç†
        """
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        words = jieba.lcut(text.lower())
        return [word.strip() for word in words if len(word.strip()) > 1]
    
    def build_index(self, documents: List[str]):
        """
        æ„å»ºç®€å•è¯å‘é‡ç´¢å¼•
        """
        self.documents = documents
        
        # æ„å»ºè¯æ±‡è¡¨
        for doc in documents:
            words = self.preprocess_text(doc)
            self.vocabulary.update(words)
        
        self.vocab_to_idx = {word: idx for idx, word in enumerate(sorted(self.vocabulary))}
        vocab_size = len(self.vocabulary)
        
        # æ„å»ºæ–‡æ¡£å‘é‡
        for doc in documents:
            words = self.preprocess_text(doc)
            word_counts = Counter(words)
            
            # åˆ›å»ºè¯é¢‘å‘é‡
            vector = np.zeros(vocab_size)
            for word, count in word_counts.items():
                if word in self.vocab_to_idx:
                    vector[self.vocab_to_idx[word]] = count
            
            # L2å½’ä¸€åŒ–
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = vector / norm
            
            self.doc_vectors.append(vector)
        
        self.doc_vectors = np.array(self.doc_vectors)
        print(f"ğŸ“Š ç®€å•å‘é‡ç»´åº¦: {self.doc_vectors.shape}")
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float, str]]:
        """
        æ‰§è¡Œç®€å•å‘é‡æ£€ç´¢
        """
        words = self.preprocess_text(query)
        word_counts = Counter(words)
        
        # åˆ›å»ºæŸ¥è¯¢å‘é‡
        query_vector = np.zeros(len(self.vocabulary))
        for word, count in word_counts.items():
            if word in self.vocab_to_idx:
                query_vector[self.vocab_to_idx[word]] = count
        
        # L2å½’ä¸€åŒ–
        norm = np.linalg.norm(query_vector)
        if norm > 0:
            query_vector = query_vector / norm
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = np.dot(self.doc_vectors, query_vector)
        
        # æ’åºå¹¶è¿”å›ç»“æœ
        sorted_indices = np.argsort(similarities)[::-1]
        
        results = []
        for i in sorted_indices[:top_k]:
            results.append((i, similarities[i], self.documents[i]))
        
        return results

def test_embedding_retrieval():
    """
    Embeddingæ£€ç´¢æµ‹è¯•ç”¨ä¾‹
    """
    print("ğŸ§  Embeddingæ£€ç´¢æµ‹è¯•ç”¨ä¾‹")
    print("=" * 50)
    
    # æµ‹è¯•æ–‡æ¡£é›†åˆ
    documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ¨¡å¼è¯†åˆ«ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯è®¡ç®—æœºç§‘å­¦å’Œäººå·¥æ™ºèƒ½çš„äº¤å‰é¢†åŸŸã€‚",
        "æ•°æ®ç§‘å­¦ç»“åˆäº†ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚",
        "Pythonåœ¨æ•°æ®åˆ†æå’Œå¯è§†åŒ–æ–¹é¢æœ‰å¾ˆå¤šä¼˜ç§€çš„åº“ã€‚",
        "TensorFlowå’ŒPyTorchæ˜¯æµè¡Œçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚",
        "æœç´¢å¼•æ“ä½¿ç”¨ä¿¡æ¯æ£€ç´¢æŠ€æœ¯æ¥æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ã€‚",
        "BM25æ˜¯ä¸€ç§ç»å…¸çš„æ–‡æ¡£æ£€ç´¢ç®—æ³•ï¼ŒåŸºäºæ¦‚ç‡æ¨¡å‹ã€‚",
        "å‘é‡ç©ºé—´æ¨¡å‹æ˜¯ä¿¡æ¯æ£€ç´¢ä¸­çš„å¦ä¸€ç§é‡è¦æ–¹æ³•ã€‚"
    ]
    
    # æµ‹è¯•TF-IDFå‘é‡æ£€ç´¢
    print("\nğŸ” TF-IDFå‘é‡æ£€ç´¢æµ‹è¯•")
    print("-" * 40)
    
    tfidf_retriever = EmbeddingRetriever(max_features=500)
    tfidf_retriever.build_index(documents)
    
    test_queries = [
        "Pythonç¼–ç¨‹è¯­è¨€",
        "æœºå™¨å­¦ä¹ ç®—æ³•",
        "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ",
        "æ•°æ®ç§‘å­¦åˆ†æ",
        "ä¿¡æ¯æ£€ç´¢æŠ€æœ¯"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\næŸ¥è¯¢ {i}: {query}")
        print("-" * 30)
        
        results = tfidf_retriever.search(query, top_k=3)
        
        for rank, (doc_idx, score, doc_content) in enumerate(results, 1):
            print(f"æ’å {rank}: (ç›¸ä¼¼åº¦: {score:.4f})")
            print(f"æ–‡æ¡£ {doc_idx}: {doc_content}")
            print()
        
        # æ˜¾ç¤ºæŸ¥è¯¢ç‰¹å¾æƒé‡
        feature_weights = tfidf_retriever.get_feature_weights(query, top_features=5)
        if feature_weights:
            print("ğŸ”‘ å…³é”®ç‰¹å¾è¯:")
            for feature, weight in feature_weights:
                print(f"  {feature}: {weight:.4f}")
            print()
    
    # æµ‹è¯•ç®€å•è¯å‘é‡æ£€ç´¢
    print("\nğŸ” ç®€å•è¯å‘é‡æ£€ç´¢æµ‹è¯•")
    print("-" * 40)
    
    simple_retriever = SimpleWordEmbeddingRetriever()
    simple_retriever.build_index(documents)
    
    query = "Pythonæ•°æ®ç§‘å­¦"
    print(f"\næŸ¥è¯¢: {query}")
    print("-" * 30)
    
    results = simple_retriever.search(query, top_k=3)
    
    for rank, (doc_idx, score, doc_content) in enumerate(results, 1):
        print(f"æ’å {rank}: (ç›¸ä¼¼åº¦: {score:.4f})")
        print(f"æ–‡æ¡£ {doc_idx}: {doc_content}")
        print()
    
    # æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("-" * 40)
    
    import time
    
    test_query = "Pythonæœºå™¨å­¦ä¹ "
    
    # TF-IDFæ£€ç´¢æ€§èƒ½
    start_time = time.time()
    for _ in range(100):
        tfidf_results = tfidf_retriever.search(test_query, top_k=5)
    tfidf_time = (time.time() - start_time) / 100 * 1000
    
    # ç®€å•å‘é‡æ£€ç´¢æ€§èƒ½
    start_time = time.time()
    for _ in range(100):
        simple_results = simple_retriever.search(test_query, top_k=5)
    simple_time = (time.time() - start_time) / 100 * 1000
    
    print(f"TF-IDFæ£€ç´¢å¹³å‡æ—¶é—´: {tfidf_time:.2f} ms")
    print(f"ç®€å•å‘é‡æ£€ç´¢å¹³å‡æ—¶é—´: {simple_time:.2f} ms")
    
    # ç»“æœè´¨é‡å¯¹æ¯”
    print("\nğŸ“Š ç»“æœè´¨é‡å¯¹æ¯”")
    print("-" * 40)
    
    print(f"\næŸ¥è¯¢: {test_query}")
    print("\nTF-IDFç»“æœ:")
    for rank, (doc_idx, score, doc_content) in enumerate(tfidf_results[:3], 1):
        print(f"  {rank}. (åˆ†æ•°: {score:.4f}) {doc_content[:50]}...")
    
    print("\nç®€å•å‘é‡ç»“æœ:")
    for rank, (doc_idx, score, doc_content) in enumerate(simple_results[:3], 1):
        print(f"  {rank}. (åˆ†æ•°: {score:.4f}) {doc_content[:50]}...")
    
    # å‘é‡ç©ºé—´å¯è§†åŒ–ä¿¡æ¯
    print("\nğŸ“ˆ å‘é‡ç©ºé—´ä¿¡æ¯")
    print("-" * 40)
    
    print(f"TF-IDFå‘é‡ç»´åº¦: {tfidf_retriever.doc_vectors.shape}")
    print(f"TF-IDFç¨€ç–åº¦: {1 - tfidf_retriever.doc_vectors.nnz / (tfidf_retriever.doc_vectors.shape[0] * tfidf_retriever.doc_vectors.shape[1]):.4f}")
    print(f"ç®€å•å‘é‡ç»´åº¦: {simple_retriever.doc_vectors.shape}")
    print(f"è¯æ±‡è¡¨å¤§å°: {len(simple_retriever.vocabulary)}")

def compare_retrieval_methods():
    """
    æ¯”è¾ƒä¸åŒæ£€ç´¢æ–¹æ³•çš„æ•ˆæœ
    """
    print("\nğŸ”„ æ£€ç´¢æ–¹æ³•å¯¹æ¯”åˆ†æ")
    print("=" * 50)
    
    documents = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ¨¡å¼è¯†åˆ«ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯è®¡ç®—æœºç§‘å­¦å’Œäººå·¥æ™ºèƒ½çš„äº¤å‰é¢†åŸŸã€‚",
        "æ•°æ®ç§‘å­¦ç»“åˆäº†ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚"
    ]
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    tfidf_retriever = EmbeddingRetriever(max_features=100)
    tfidf_retriever.build_index(documents)
    
    simple_retriever = SimpleWordEmbeddingRetriever()
    simple_retriever.build_index(documents)
    
    test_queries = [
        "Pythonç¼–ç¨‹",
        "æœºå™¨å­¦ä¹ äººå·¥æ™ºèƒ½",
        "æ•°æ®ç§‘å­¦ç»Ÿè®¡"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        print("-" * 30)
        
        # TF-IDFç»“æœ
        tfidf_results = tfidf_retriever.search(query, top_k=2)
        print("TF-IDF Top-2:")
        for rank, (doc_idx, score, _) in enumerate(tfidf_results, 1):
            print(f"  {rank}. æ–‡æ¡£{doc_idx} (åˆ†æ•°: {score:.4f})")
        
        # ç®€å•å‘é‡ç»“æœ
        simple_results = simple_retriever.search(query, top_k=2)
        print("\nç®€å•å‘é‡ Top-2:")
        for rank, (doc_idx, score, _) in enumerate(simple_results, 1):
            print(f"  {rank}. æ–‡æ¡£{doc_idx} (åˆ†æ•°: {score:.4f})")
        
        print()

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import jieba
        import sklearn
        import numpy as np
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("è¯·å®‰è£…: pip install jieba scikit-learn numpy")
        exit(1)
    
    test_embedding_retrieval()
    compare_retrieval_methods()